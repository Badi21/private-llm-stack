# ğŸš€ Private LLM Stack

<div align="center">

![GitHub stars](https://img.shields.io/github/stars/badi21/private-llm-stack?style=social)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Docker Pulls](https://img.shields.io/badge/docker%20pulls-compatible-blue)
[![Twitter Follow](https://img.shields.io/twitter/follow/badrmind?style=social)](https://twitter.com/badrmind)

**Despliega tu propia plataforma de IA conversacional segura y privada en minutos**

[English](./README_EN.md) | [EspaÃ±ol](./README.md)
</div>

## ğŸ“‹ Contenido

- [ğŸ” VisiÃ³n general](#-visiÃ³n-general)
- [âœ¨ CaracterÃ­sticas](#-caracterÃ­sticas)
- [ğŸ–¥ï¸ Requisitos del sistema](#ï¸-requisitos-del-sistema)
- [ğŸ“¦ InstalaciÃ³n rÃ¡pida](#-instalaciÃ³n-rÃ¡pida)
- [ğŸ› ï¸ ConfiguraciÃ³n avanzada](#ï¸-configuraciÃ³n-avanzada)
- [ğŸ¤– Modelos compatibles](#-modelos-compatibles)
- [ğŸ” Seguridad](#-seguridad)
- [ğŸ”„ ActualizaciÃ³n](#-actualizaciÃ³n)
- [â“ Preguntas frecuentes](#-preguntas-frecuentes)
- [ğŸ‘¥ ContribuciÃ³n](#-contribuciÃ³n)
- [ğŸ“œ Licencia](#-licencia)

## ğŸ” VisiÃ³n general

**Private LLM Stack** te permite desplegar tu propio **ChatGPT autÃ³nomo, privado y seguro** en cualquier servidor VPS, utilizando modelos de lenguaje avanzados como LLaMA, Mistral y otros. Todo con una interfaz web moderna similar a ChatGPT, protegida con HTTPS y autenticaciÃ³n.

> **Â¿Por quÃ© usar Private LLM Stack?** MantÃ©n tus datos sensibles bajo tu control, evita dependencias externas, y opera completamente sin conexiÃ³n a internet tras la instalaciÃ³n inicial.

## âœ¨ CaracterÃ­sticas

- **ğŸ§  IA localmente instalada**: Ollama AI runtime con soporte para mÃºltiples modelos de lenguaje
- **ğŸ­ MÃºltiples modelos segÃºn tu hardware**: Desde opciones ligeras (2GB RAM) hasta modelos avanzados (16GB+)
- **ğŸ–Œï¸ Interfaz moderna web**: Open WebUI con diseÃ±o similar a ChatGPT, compatible con mÃ³vil
- **ğŸ”’ Seguridad integrada**: NGINX como proxy inverso, HTTPS con SSL automÃ¡tico y autenticaciÃ³n bÃ¡sica
- **ğŸš€ InstalaciÃ³n en un paso**: Script automatizado que configura todo el stack
- **ğŸ“± Acceso multiplataforma**: Accede desde cualquier dispositivo a travÃ©s de tu dominio seguro
- **ğŸ”„ API compatible**: Expone endpoints compatibles con la API de OpenAI para integraciones
- **ğŸ’¾ Sin datos en la nube**: Toda la informaciÃ³n se procesa y almacena localmente
- **ğŸ› ï¸ FÃ¡cil mantenimiento**: Comandos sencillos para actualizar y gestionar el sistema

## ğŸ–¥ï¸ Requisitos del sistema

| Recurso | Requisito mÃ­nimo | Recomendado |
|---------|-----------------|-------------|
| CPU | 2 vCPU | 4+ vCPU (AVX2 soportado) |
| RAM | 2GB (modelos ligeros) | 8-16GB (modelos avanzados) |
| Almacenamiento | 10GB libre | 20GB+ SSD |
| SO | Ubuntu 20.04/22.04 | Ubuntu 22.04+ o Debian 11+ |
| Red | Puerto 80, 443 abiertos | Velocidad de subida estable |
| Dominio | Requerido para SSL | Con DNS configurado hacia tu servidor |

## ğŸ“¦ InstalaciÃ³n rÃ¡pida

```bash
# 1. Descarga el script de instalaciÃ³n
curl -sSL https://raw.githubusercontent.com/badi21/private-llm-stack/main/install.sh -o install.sh

# 2. Haz el script ejecutable
chmod +x install.sh

# 3. Ejecuta el instalador
./install.sh
```

El script te guiarÃ¡ a travÃ©s de un proceso interactivo para elegir tu modelo, configurar tu dominio y establecer credenciales de acceso.

## ğŸ› ï¸ ConfiguraciÃ³n avanzada

Para personalizar la instalaciÃ³n, puedes ejecutar el script con opciones:

```bash
./install.sh --model mistral:7b-instruct --domain ia.tudominio.com --username admin
```

ParÃ¡metros disponibles:
- `--model`: Selecciona el modelo de IA (ver [modelos compatibles](#-modelos-compatibles))
- `--domain`: Especifica el dominio para acceso web
- `--username`: Define el nombre de usuario para la autenticaciÃ³n
- `--port`: Cambia el puerto predeterminado (por defecto: 3000)
- `--no-ssl`: Omite la configuraciÃ³n de SSL (no recomendado)
- `--help`: Muestra todas las opciones disponibles

## ğŸ¤– Modelos compatibles

Private LLM Stack es compatible con todos los modelos de Ollama. Recomendamos:

| Modelo | RAM mÃ­nima | Caso de uso |
|--------|------------|------------|
| phi | 2GB | Chatbot bÃ¡sico, dispositivos de recursos limitados |
| gemma:2b | 4GB | Asistente general, buena relaciÃ³n rendimiento/recursos |
| tinyllama | 2GB | Asistente ligero para VPS econÃ³micos |
| llama2:7b-chat | 8GB | ConversaciÃ³n avanzada, mejor comprensiÃ³n |
| mistral:7b-instruct | 8GB | Excelente rendimiento general, instrucciones complejas |
| neural-chat | 12-16GB | Experiencia conversacional premium, alta precisiÃ³n |

> ğŸ’¡ **Consejo**: Puedes instalar mÃºltiples modelos y cambiar entre ellos segÃºn tus necesidades.

## ğŸ” Seguridad

- **HTTPS automÃ¡tico**: Certificados Let's Encrypt autogestionados
- **AutenticaciÃ³n**: ProtecciÃ³n con usuario/contraseÃ±a
- **Aislamiento**: Docker para contenerizaciÃ³n segura
- **ComunicaciÃ³n local**: Los datos nunca salen de tu servidor

### Mejores prÃ¡cticas de seguridad

- Cambia regularmente la contraseÃ±a de autenticaciÃ³n
- Actualiza el sistema y componentes regularmente
- Utiliza un firewall para limitar el acceso al servidor
- Considera configurar una VPN para acceso remoto mÃ¡s seguro


## ğŸ”„ ActualizaciÃ³n

Para actualizar el sistema a la Ãºltima versiÃ³n:

```bash
cd private-llm-stack
git pull
./update.sh
```

## â“ Preguntas frecuentes

<details>
<summary><b>Â¿Es posible usar este sistema sin conexiÃ³n a internet?</b></summary>
SÃ­, una vez instalado y descargados los modelos, el sistema puede funcionar completamente sin conexiÃ³n.
</details>

<details>
<summary><b>Â¿CÃ³mo puedo aÃ±adir mÃ¡s usuarios?</b></summary>
Puedes aÃ±adir mÃ¡s usuarios editando el archivo de autenticaciÃ³n de NGINX:

```bash
sudo htpasswd /etc/nginx/.htpasswd nuevo_usuario
```
</details>

<details>
<summary><b>Â¿Funciona en una Raspberry Pi?</b></summary>
SÃ­, con modelos ligeros (phi, tinyllama) puede funcionar en Raspberry Pi 4 con 4GB+ RAM.
</details>

<details>
<summary><b>Â¿Puedo usar esto para fines comerciales?</b></summary>
SÃ­, pero debes verificar las licencias de los modelos especÃ­ficos que utilices, ya que varÃ­an.
</details>

## ğŸ‘¥ ContribuciÃ³n

Â¡Las contribuciones son bienvenidas! Si quieres mejorar Private LLM Stack:

1. Haz fork del repositorio
2. Crea una rama para tu funcionalidad (`git checkout -b feature/amazing-feature`)
3. Haz commit de tus cambios (`git commit -m 'Add: amazing feature'`)
4. Push a la rama (`git push origin feature/amazing-feature`)
5. Abre un Pull Request

TambiÃ©n puedes [reportar problemas](https://github.com/badi21/private-llm-stack/issues) o [sugerir mejoras](https://github.com/badi21/private-llm-stack/discussions).

## ğŸ“œ Licencia

Este proyecto estÃ¡ bajo la licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

<div align="center">
<p>Â¿Te gusta este proyecto? Â¡Dale una estrella! â­</p>
<p>Creado con ğŸ’» por <a href="https://github.com/badi21">Badar - Ø¨Ø¯Ø± Ø¯ÙŠÙ† | Software Engineer & Inspiration</a></p>
</div>